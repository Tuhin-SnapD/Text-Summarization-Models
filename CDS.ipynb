{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "eTG7KWVOP_aZ"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSTScWFlRbK4",
        "outputId": "8d27d987-8ebd-46db-f181-21f8dbe8b877"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocess a given text by tokenizing, removing stop words, and lemmatizing the words.\n",
        "    \"\"\"\n",
        "    # tokenize the text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # remove stop words and lemmatize the words in each sentence\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    preprocessed_sentences = []\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence.lower())\n",
        "        filtered_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "        preprocessed_sentence = \" \".join(filtered_words)\n",
        "        preprocessed_sentences.append(preprocessed_sentence)\n",
        "\n",
        "    return preprocessed_sentences"
      ],
      "metadata": {
        "id": "_FrNEG0fQ_fR"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_similarity(sentence1, sentence2):\n",
        "    \"\"\"\n",
        "    Compute the similarity score between two sentences using TF-IDF.\n",
        "    \"\"\"\n",
        "    tfidf = TfidfVectorizer().fit_transform([sentence1, sentence2])\n",
        "    similarity_score = (tfidf * tfidf.T).A[0, 1]\n",
        "    return similarity_score"
      ],
      "metadata": {
        "id": "QOq-xHNrQ_3-"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_minimum_cds(graph):\n",
        "    \"\"\"\n",
        "    Find the minimum Connected Dominating Set (CDS) of a graph using a greedy algorithm.\n",
        "    \"\"\"\n",
        "    cds = set() # initialize CDS to empty set\n",
        "    nodes = set(graph.nodes()) # get all nodes in the graph\n",
        "\n",
        "    while nodes:\n",
        "        max_degree_node = max(nodes, key=lambda n: graph.degree(n)) # find node with highest degree\n",
        "        cds.add(max_degree_node) # add node to CDS\n",
        "        nodes.discard(max_degree_node) # remove node from remaining nodes\n",
        "        neighbors = set(graph.neighbors(max_degree_node)) # get all neighbors of the node\n",
        "        nodes.difference_update(neighbors) # remove neighbors from remaining nodes\n",
        "\n",
        "    return cds"
      ],
      "metadata": {
        "id": "85qyfQEvRCIE"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_text(text, summary_size, threshold=0.1):\n",
        "    \"\"\"\n",
        "    Summarize a given text using minimum Connected Dominating Set (CDS).\n",
        "    \"\"\"\n",
        "    # preprocess the text\n",
        "    preprocessed_sentences = preprocess_text(text)\n",
        "\n",
        "    # create graph from preprocessed sentences\n",
        "    graph = nx.Graph()\n",
        "    for i, sentence in enumerate(preprocessed_sentences):\n",
        "        for j in range(i+1, len(preprocessed_sentences)):\n",
        "            similarity_score = compute_similarity(sentence, preprocessed_sentences[j]) # compute similarity score between two sentences\n",
        "            if similarity_score > threshold:\n",
        "                graph.add_edge(i, j, weight=similarity_score)\n",
        "\n",
        "    # find minimum CDS of the graph\n",
        "    cds = find_minimum_cds(graph)\n",
        "\n",
        "    # sort the CDS nodes based on their occurrence order in the original text\n",
        "    summary_nodes = sorted(list(cds))\n",
        "\n",
        "    # create summary by concatenating the selected sentences\n",
        "    summary = \". \".join([sent_tokenize(text)[i] for i in summary_nodes][:summary_size])\n",
        "\n",
        "    return summary"
      ],
      "metadata": {
        "id": "jA-5S0DxREBP"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "Astronomers have used the James Webb Space Telescope to peer back in time to the early days of the universe — and they spotted something unexpected.\n",
        "The space observatory revealed six massive galaxies that existed between 500 million and 700 million years after the big bang that created the universe. The discovery is completely upending existing theories about the origins of galaxies, according to a new study published Wednesday in the journal Nature.\n",
        "“These objects are way more massive​ than anyone expected,” said study coauthor Joel Leja, assistant professor of astronomy and astrophysics at Penn State University, in a statement. “We expected only to find tiny, young, baby galaxies at this point in time, but we’ve discovered galaxies as mature as our own in what was previously understood to be the dawn of the universe.”\n",
        "The telescope observes the universe in infrared light, which is invisible to the human eye, and is capable of detecting the faint light from ancient stars and galaxies. By peering into the distant universe, the observatory can essentially see back in time up to about 13.5 billion years ago. (Scientists have determined the universe is about 13.7 billion years old.)\n",
        "\"\"\"\n",
        "\n",
        "summary_size = 3 # number of sentences in the summary\n",
        "summary = summarize_text(text, summary_size)\n",
        "\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTepjNBWRGUf",
        "outputId": "3000a3d3-e0da-4a45-ff36-9fc531c659bf"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Astronomers have used the James Webb Space Telescope to peer back in time to the early days of the universe — and they spotted something unexpected.. The space observatory revealed six massive galaxies that existed between 500 million and 700 million years after the big bang that created the universe.\n"
          ]
        }
      ]
    }
  ]
}