{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Pipeline\n",
        "Text summarization of news articles using **transformer pipeline** has several advantages and disadvantages:\n",
        "\n",
        "### Pros:\n",
        "\n",
        "* Time-saving: Summarization of news articles using transformer pipeline can save a lot of time for readers who want to quickly get an idea about the news without reading the entire article.\n",
        "\n",
        "* Better comprehension: Summaries generated by transformer pipeline models are often well-written, coherent and provide an accurate representation of the original text, which can help readers better understand the main points of the article.\n",
        "\n",
        "* Reduced bias: Transformer models are trained on large amounts of data, which helps to reduce the bias that may exist in human-written summaries.\n",
        "\n",
        "* Multilingual Support: Transformer models can support summarization of news articles in multiple languages, making it easier for readers to stay informed about news from around the world.\n",
        "\n",
        "###Cons:\n",
        "\n",
        "* Loss of details: One of the major drawbacks of using a text summarization model is that it can sometimes lead to loss of important details, nuances, and context of the original text, which can be critical in certain types of news articles.\n",
        "\n",
        "* Limited flexibility: Transformer models are trained on a large dataset and may not be able to capture the unique writing style of an individual news source, resulting in generic summaries.\n",
        "\n",
        "* Model Complexity: Transformer models require significant computing resources and expertise to train and maintain, which can be a barrier for smaller news organizations or individuals.\n",
        "\n",
        "* Dependence on Training Data: The quality of the summary generated by a transformer model is highly dependent on the quality and relevance of the training data used to train the model. If the training data is biased or limited, the quality of the summaries may be compromised.\n",
        "\n",
        "Overall, while text summarization using transformer pipeline has some limitations, it has the potential to significantly improve the efficiency and accessibility of news article.\n",
        "\n",
        "These are the scores we achieved:\n",
        "\n",
        "    ROUGE Score:\n",
        "    Precision: 0.938\n",
        "    Recall: 0.397\n",
        "    F1-Score: 0.558\n",
        "\n",
        "    BLEU Score: 0.795\n",
        "\n",
        "## References\n",
        "Here are some research papers related to Transformer-based pipelines for text summarization:\n",
        "\n",
        "1. \"PreSumm: Simple and Effective Multi-Document Summarization\" by J. Zhang, Y. Chen, J. Guo, and D. Yin, in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL), 2019.\n",
        "\n",
        "1. \"Fine-tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping\" by C. Raffel and N. Shazeer, in Proceedings of the 7th International Conference on Learning Representations (ICLR), 2019.\n",
        "\n",
        "1. \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\" by C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, in Journal of Machine Learning Research (JMLR), 2020.\n",
        "\n",
        "These papers explore various Transformer-based models for text summarization, such as PreSumm, BART, and T5. They also discuss different techniques for fine-tuning and optimizing these models, including weight initialization, early stopping, and data orders.\n",
        "\n",
        "The Transformer architecture is a type of neural network that has been highly successful in natural language processing tasks, including text summarization. Transformer-based models typically use pre-trained language models, such as BERT or GPT, as a starting point and then fine-tune them on a specific summarization task using large amounts of data.\n",
        "\n",
        "The papers suggest that Transformer-based pipelines are highly effective for text summarization, achieving state-of-the-art results on a wide range of benchmark datasets. These models are highly flexible and can be adapted to different summarization tasks and domains with minimal modification."
      ],
      "metadata": {
        "id": "lSGwCXXcdwbs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzfJR8eaGiXc",
        "outputId": "a907fc98-c71e-4d72-d677-93b5fc24d83d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.26.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (0.1.97)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U transformers\n",
        "!pip install sentencepiece\n",
        "import torch\n",
        "import json \n",
        "from transformers import pipeline   "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = pipeline(\"summarization\")\n",
        "text =\"\"\"\n",
        "India's Health Ministry has announced that the country's COVID-19 vaccination drive will now be expanded to include people over the age of 60 and those over 45 with co-morbidities. The move is expected to cover an additional 270 million people, making it one of the largest vaccination drives in the world.The decision was taken after a meeting of the National Expert Group on Vaccine Administration for COVID-19 (NEGVAC), which recommended the expansion of the vaccination program. The NEGVAC also suggested that private hospitals may be allowed to administer the vaccine, although the details of this are yet to be finalized.India began its vaccination drive in mid-January, starting with healthcare and frontline workers. Since then, over 13 million doses have been administered across the country. However, the pace of the vaccination drive has been slower than expected, with concerns raised over vaccine hesitancy and logistical challenges.The expansion of the vaccination drive to include the elderly and those with co-morbidities is a major step towards achieving herd immunity and controlling the spread of the virus in India. The Health Ministry has also urged eligible individuals to come forward and get vaccinated at the earliest.India has reported over 11 million cases of COVID-19, making it the second-worst affected country in the world after the United States. The country's daily case count has been declining in recent weeks, but experts have warned that the pandemic is far from over and that precautions need to be maintained.\n",
        "In summary, India's Health Ministry has announced that the country's COVID-19 vaccination drive will be expanded to include people over 60 and those over 45 with co-morbidities, covering an additional 270 million people. The decision was taken after a meeting of the National Expert Group on Vaccine Administration for COVID-19, and is a major step towards achieving herd immunity and controlling the spread of the virus in India\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jy6P0V2UbcJu",
        "outputId": "062f3e3e-3134-47ab-9acb-4dd770f55c51"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summ=summarizer(text)\n",
        "for sentence in summ:\n",
        "    str1 = \"\"\n",
        "    str1 += str(sentence)\n",
        "    print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kb5JbitpcIIv",
        "outputId": "cfd7f57b-6d93-4b59-9e94-4bf775819be3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'summary_text': \" India's COVID-19 vaccination drive will now be expanded to include people over 60 and those over 45 with co-morbidities . The move is expected to cover an additional 270 million people, making it one of the largest vaccination drives in the world . India began its vaccination drive in mid-January, starting with healthcare and frontline workers . The country's daily case count has been declining in recent weeks, but experts warn that the pandemic is far from over .\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn\n",
        "!pip install rouge\n",
        "!pip install nltk\n",
        "from rouge import Rouge \n",
        "import nltk\n",
        "import nltk.translate.bleu_score as bleu\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize, sent_tokenize "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbPDGrpacqcL",
        "outputId": "d513e317-6c13-44ca-9039-416a9d7f1dcb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (1.2.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: rouge in /usr/local/lib/python3.9/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from rouge) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk) (1.2.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = Rouge()\n",
        "scores = rouge.get_scores(str1, text)\n",
        "print(\"ROUGE Score:\")\n",
        "print(\"Precision: {:.3f}\".format(scores[0]['rouge-1']['p']))\n",
        "print(\"Recall: {:.3f}\".format(scores[0]['rouge-1']['r']))\n",
        "print(\"F1-Score: {:.3f}\".format(scores[0]['rouge-1']['f']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Aq9DO9Uc9iv",
        "outputId": "593151d8-3e79-49ed-d7b7-f19b218f376f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE Score:\n",
            "Precision: 0.938\n",
            "Recall: 0.397\n",
            "F1-Score: 0.558\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def summary_to_sentences(summ):\n",
        "    # Split the summary into sentences using the '.' character as a separator\n",
        "    sentences = summ.split('.')\n",
        "    \n",
        "    # Convert each sentence into a list of words\n",
        "    sentence_lists = [sentence.split() for sentence in sentences]\n",
        "    \n",
        "    return sentence_lists\n",
        "\n",
        "def paragraph_to_wordlist(paragraph):\n",
        "    # Split the paragraph into words using whitespace as a separator\n",
        "    words = paragraph.split()\n",
        "    return words\n",
        "\n",
        "reference_paragraph = text\n",
        "reference_summary = summary_to_sentences(reference_paragraph)\n",
        "predicted_paragraph = str1\n",
        "predicted_summary = paragraph_to_wordlist(predicted_paragraph)\n",
        "\n",
        "score = sentence_bleu(reference_summary, predicted_summary)\n",
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDS2HQUhdETJ",
        "outputId": "bdb9c319-0d0a-4cb1-f6cf-292650cc1034"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7945385996828465\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"BLEU Score: {:.3f}\".format(score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xr8Zl2K8dP2U",
        "outputId": "4e30c3f2-734a-4ede-c0f6-4dac056f7709"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score: 0.795\n"
          ]
        }
      ]
    }
  ]
}