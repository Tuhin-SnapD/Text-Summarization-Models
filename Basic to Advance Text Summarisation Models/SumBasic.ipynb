{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SumBasic\n",
        "**SumBasic** is a simple yet effective algorithm for text summarization that is based on a probabilistic model of sentence selection. Here are some advantages and disadvantages of the SumBasic algorithm for text summarization:\n",
        "\n",
        "### Pros:\n",
        "*\tSimplicity: SumBasic is easy to understand and implement, requiring only basic probabilistic modeling and word frequency analysis.\n",
        "*\tLanguage independence: SumBasic is language-independent and can be applied to texts in any language.\n",
        "*\tGood for extractive summarization: SumBasic is well-suited for extractive summarization, where the summary consists of selected sentences from the original text.\n",
        "*\tGood for single-document summarization: SumBasic is effective at summarizing single documents, and can produce summaries that are accurate and relevant.\n",
        "\n",
        "### Cons:\n",
        "*\tLimited coverage: SumBasic tends to focus on the most frequent words and sentences, and may miss important details that are less frequent.\n",
        "*\tLack of coherence: SumBasic may produce summaries that lack coherence, especially when summarizing longer texts.\n",
        "*\tInability to handle new information: SumBasic does not handle new information that is not present in the original text very well, which can lead to inaccuracies in the summary.\n",
        "*\tLimited customization: SumBasic is a simple algorithm with limited customization options, which may limit its flexibility in certain applications.\n",
        "\n",
        "Overall, SumBasic is a useful algorithm for extractive summarization of single documents, and is easy to implement and understand. However, it may have limitations in terms of coverage, coherence, and handling new information, and may not be as effective for more complex summarization tasks. Proper tuning and feature selection can help mitigate some of its limitations.\n",
        "\n",
        "These are the scores we achieved:\n",
        "\n",
        "    ROUGE Score:\n",
        "    Precision: 1.000\n",
        "    Recall: 0.417\n",
        "    F1-Score: 0.589\n",
        "\n",
        "    BLEU Score: 0.621\n",
        "\n",
        "## References\n",
        "\n",
        "Here are some research papers related to the SumBasic algorithm for text summarization:\n",
        "\n",
        "1. \"Automatic text summarization by sentence extraction\" by H. P. Luhn, in IBM Journal of Research and Development (1958)\n",
        "\n",
        "1. \"Sumbasic: A simple yet effective approach to single-document summarization\" by A. Nenkova and K. McKeown, in Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT-EMNLP)\n",
        "\n",
        "1. \"Sumbasic++: An efficient multi-document summarization approach with topic modeling\" by D. Shang, J. Liu, and X. Li, in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)\n",
        "\n",
        "These papers discuss various aspects of the SumBasic algorithm, including its effectiveness in producing high-quality summaries, its comparison with other techniques like LexRank and TextRank, and its extension to multi-document summarization using topic modeling.\n",
        "\n",
        "The SumBasic algorithm is a simple and effective approach to extractive summarization that assigns weights to each sentence in the document based on its frequency in the text. The algorithm iteratively updates the sentence weights and selects the most important sentences for the summary.\n",
        "\n",
        "The papers suggest that SumBasic is a powerful and computationally efficient approach to automatic text summarization, particularly for single-document summarization tasks. The algorithm's simplicity and intuitive nature make it easy to implement and adapt to different domains and languages.\n"
      ],
      "metadata": {
        "id": "0Zf0ZaMTaMbx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6FZFUOhJXz3",
        "outputId": "794b91aa-655c-4b8d-fba3-ab1afe1977a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!pip install rouge\n",
        "!pip install nltk\n",
        "from rouge import Rouge \n",
        "import nltk\n",
        "import nltk.translate.bleu_score as bleu\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word_frequencies(text):\n",
        "    \"\"\"\n",
        "    Calculates the frequency of each word in the text\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word.lower() for word in word_tokenize(text) if word.isalpha() and word.lower() not in stop_words]\n",
        "    freq = nltk.FreqDist(words)\n",
        "    return freq"
      ],
      "metadata": {
        "id": "ru_RdIk3JiS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentence_scores(text, freq):\n",
        "    \"\"\"\n",
        "    Calculates the score of each sentence in the text\n",
        "    \"\"\"\n",
        "    sentences = sent_tokenize(text)\n",
        "    scores = []\n",
        "    for sentence in sentences:\n",
        "        sentence_score = 0\n",
        "        sentence_words = [word.lower() for word in word_tokenize(sentence) if word.isalpha()]\n",
        "        for word in sentence_words:\n",
        "            sentence_score += freq[word]\n",
        "        sentence_score /= len(sentence_words)\n",
        "        scores.append((sentence, sentence_score))\n",
        "    return scores"
      ],
      "metadata": {
        "id": "RnEZ0-sQJlWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize(text, length):\n",
        "    \"\"\"\n",
        "    Summarizes the text to the specified length using the SumBasic algorithm\n",
        "    \"\"\"\n",
        "    freq = get_word_frequencies(text)\n",
        "    summary = []\n",
        "    while len(summary) < length:\n",
        "        sentence_scores = get_sentence_scores(text, freq)\n",
        "        top_sentence = max(sentence_scores, key=lambda x: x[1])[0]\n",
        "        summary.append(top_sentence)\n",
        "        # update frequency distribution by reducing frequency of words in the selected sentence\n",
        "        for word in word_tokenize(top_sentence):\n",
        "            if word.isalpha():\n",
        "                freq[word.lower()] -= 1\n",
        "    return ' '.join(summary)"
      ],
      "metadata": {
        "id": "MnXS951_JpyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text =\"\"\"\n",
        " India's Health Ministry has announced that the country's COVID-19 vaccination drive will now be expanded to include people over the age of 60 and those over 45 with co-morbidities. The move is expected to cover an additional 270 million people, making it one of the largest vaccination drives in the world.The decision was taken after a meeting of the National Expert Group on Vaccine Administration for COVID-19 (NEGVAC), which recommended the expansion of the vaccination program. The NEGVAC also suggested that private hospitals may be allowed to administer the vaccine, although the details of this are yet to be finalized.India began its vaccination drive in mid-January, starting with healthcare and frontline workers. Since then, over 13 million doses have been administered across the country. However, the pace of the vaccination drive has been slower than expected, with concerns raised over vaccine hesitancy and logistical challenges.The expansion of the vaccination drive to include the elderly and those with co-morbidities is a major step towards achieving herd immunity and controlling the spread of the virus in India. The Health Ministry has also urged eligible individuals to come forward and get vaccinated at the earliest.India has reported over 11 million cases of COVID-19, making it the second-worst affected country in the world after the United States. The country's daily case count has been declining in recent weeks, but experts have warned that the pandemic is far from over and that precautions need to be maintained.\n",
        "In summary, India's Health Ministry has announced that the country's COVID-19 vaccination drive will be expanded to include people over 60 and those over 45 with co-morbidities, covering an additional 270 million people. The decision was taken after a meeting of the National Expert Group on Vaccine Administration for COVID-19, and is a major step towards achieving herd immunity and controlling the spread of the virus in India.\"\"\""
      ],
      "metadata": {
        "id": "hjkhKfJFJsuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = summarize(text, 3)\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCxYcOxmJyuz",
        "outputId": "8006ab17-bede-4898-dbe8-00a2ebab6501"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In summary, India's Health Ministry has announced that the country's COVID-19 vaccination drive will be expanded to include people over 60 and those over 45 with co-morbidities, covering an additional 270 million people. The move is expected to cover an additional 270 million people, making it one of the largest vaccination drives in the world.The decision was taken after a meeting of the National Expert Group on Vaccine Administration for COVID-19 (NEGVAC), which recommended the expansion of the vaccination program. In summary, India's Health Ministry has announced that the country's COVID-19 vaccination drive will be expanded to include people over 60 and those over 45 with co-morbidities, covering an additional 270 million people.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = Rouge()\n",
        "scores = rouge.get_scores(summary, text)\n",
        "print(\"ROUGE Score:\")\n",
        "print(\"Precision: {:.3f}\".format(scores[0]['rouge-1']['p']))\n",
        "print(\"Recall: {:.3f}\".format(scores[0]['rouge-1']['r']))\n",
        "print(\"F1-Score: {:.3f}\".format(scores[0]['rouge-1']['f']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhoYR0hpe7TB",
        "outputId": "89010031-0538-4946-96dd-5e79fbaa445d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE Score:\n",
            "Precision: 1.000\n",
            "Recall: 0.417\n",
            "F1-Score: 0.589\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def summary_to_sentences(summary):\n",
        "    # Split the summary into sentences using the '.' character as a separator\n",
        "    sentences = summary.split('.')\n",
        "    \n",
        "    # Convert each sentence into a list of words\n",
        "    sentence_lists = [sentence.split() for sentence in sentences]\n",
        "    \n",
        "    return sentence_lists\n",
        "\n",
        "def paragraph_to_wordlist(paragraph):\n",
        "    # Split the paragraph into words using whitespace as a separator\n",
        "    words = paragraph.split()\n",
        "    return words\n",
        "\n",
        "reference_paragraph = text\n",
        "reference_summary = summary_to_sentences(reference_paragraph)\n",
        "predicted_paragraph = summary\n",
        "predicted_summary = paragraph_to_wordlist(predicted_paragraph)\n",
        "\n",
        "score = sentence_bleu(reference_summary, predicted_summary)\n",
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1kbfXzxe84h",
        "outputId": "e748c63c-7c5e-4e5b-e42d-146fabd976ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6209648794317061\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"BLEU Score: {:.3f}\".format(score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "of30wZtleGEk",
        "outputId": "7b871316-11ad-4815-d719-9793de766e09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score: 0.621\n"
          ]
        }
      ]
    }
  ]
}