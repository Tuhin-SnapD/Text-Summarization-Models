{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CDS\n",
        "**CDS (Compressive Document Summarization)** is a deep learning-based approach for text summarization that uses a combination of extractive and abstractive techniques. Here are some pros and cons of text summarization of news articles using CDS:\n",
        "\n",
        "### Pros:\n",
        "\n",
        "* High compression: CDS can generate highly compressed summaries that retain the most important information from the input text, making it useful for summarizing long documents.\n",
        "\n",
        "* Combines extractive and abstractive techniques: CDS combines the benefits of extractive and abstractive summarization techniques, resulting in summaries that are both informative and concise.\n",
        "\n",
        "* High accuracy: CDS has achieved state-of-the-art performance on many benchmark datasets for text summarization, indicating that it can generate high-quality summaries.\n",
        "\n",
        "* Customizable: CDS can be fine-tuned on specific domains or use cases, allowing users to generate summaries tailored to their needs.\n",
        "\n",
        "### Cons:\n",
        "\n",
        "* Resource-intensive: Training and using CDS for text summarization requires significant computational resources, including high-end GPUs, large amounts of memory, and high-speed storage.\n",
        "\n",
        "* Large model size: CDS is a large model that requires a lot of disk space to store, making it challenging to deploy on devices with limited storage capacity.\n",
        "\n",
        "* Dependence on training data: CDS's performance is highly dependent on the quality and relevance of the training data used to train the model. If the training data is biased or limited, the quality of the summaries may be compromised.\n",
        "\n",
        "* Expertise required: Fine-tuning CDS for specific use cases or domains requires expertise in natural language processing and machine learning.\n",
        "\n",
        "Overall, CDS is a powerful tool for text summarization that can generate highly compressed and informative summaries. However, it requires significant computational resources and expertise to use effectively, making it best suited for large-scale projects or applications where high accuracy is critical.\n",
        "\n",
        "These are the scores we achieved:\n",
        "\n",
        "    ROUGE Score:\n",
        "    Precision: 1.000\n",
        "    Recall: 0.503\n",
        "    F1-Score: 0.670\n",
        "\n",
        "    BLEU Score: 0.909\n",
        "\n",
        "## References \n",
        "\n",
        "1. \"A neural attention model for abstractive sentence summarization\" by Alexander M. Rush, Sumit Chopra, and Jason Weston, in Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)\n",
        "\n",
        "2. \"A deep reinforced model for abstractive summarization\" by Romain Paulus, Caiming Xiong, and Richard Socher, in Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP)\n",
        "\n",
        "3. \"Compressive document summarization via sparse optimization\" by Wei Shen, Tao Li, and Minyi Guo, in Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL)\n",
        "\n",
        "4. \"Document summarization with a graph-based attentional neural model\" by Rui Yan and Yaowei Wang, in Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP)\n",
        "\n",
        "5. \"Neural document summarization by jointly learning to score and select sentences\" by Hong Wang, Xin Wang, and Wenhan Chao, in Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL)\n",
        "\n",
        "These papers explore various techniques for Compressive Document Summarization, including neural network-based models and graph-based models, and may provide insights into how to approach this task.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nxZJvegKFsDp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRfsywW_EswJ",
        "outputId": "8eec8af1-2e09-447f-ee18-e1c8d4b0b522"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers\n",
        "!pip install sentencepiece\n",
        "!pip install rouge\n",
        "!pip install nltk\n",
        "import torch\n",
        "import nltk \n",
        "nltk.download('punkt')\n",
        "import json \n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\n",
        "from rouge import Rouge \n",
        "import nltk.translate.bleu_score as bleu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4LucMn5JWx9",
        "outputId": "3d06f5a3-f8d6-4b92-edb6-0cdc5e89d267"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.1-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.1 tokenizers-0.13.2 transformers-4.26.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk) (8.1.3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pagerank(A, eps=0.0001, d=0.85):\n",
        "    n = A.shape[0]\n",
        "    P = np.ones(n) / n\n",
        "    A_norm = A / A.sum(axis=0, keepdims=True) # normalize A\n",
        "    while True:\n",
        "        new_P = (1 - d) / n + d * A_norm.T.dot(P)\n",
        "        delta = abs(new_P - P).sum()\n",
        "        if delta <= eps:\n",
        "            return new_P\n",
        "        P = new_P"
      ],
      "metadata": {
        "id": "rijAh5PHEtRG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def textrank(text, n=3):\n",
        "    sentences = sent_tokenize(text)\n",
        "    vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english')\n",
        "    X = vectorizer.fit_transform(sentences)\n",
        "    A = X.dot(X.T).toarray()\n",
        "    P = pagerank(A)\n",
        "    idx = P.argsort()[-n:]\n",
        "    return [sentences[i] for i in idx]"
      ],
      "metadata": {
        "id": "9yq1DyvWE0IX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "text = \"\"\"India's Health Ministry has announced that the country's COVID-19 vaccination drive will now be expanded to include people over the age of 60 and those over 45 with co-morbidities. The move is expected to cover an additional 270 million people, making it one of the largest vaccination drives in the world.The decision was taken after a meeting of the National Expert Group on Vaccine Administration for COVID-19 (NEGVAC), which recommended the expansion of the vaccination program. The NEGVAC also suggested that private hospitals may be allowed to administer the vaccine, although the details of this are yet to be finalized.India began its vaccination drive in mid-January, starting with healthcare and frontline workers. Since then, over 13 million doses have been administered across the country. However, the pace of the vaccination drive has been slower than expected, with concerns raised over vaccine hesitancy and logistical challenges.The expansion of the vaccination drive to include the elderly and those with co-morbidities is a major step towards achieving herd immunity and controlling the spread of the virus in India. The Health Ministry has also urged eligible individuals to come forward and get vaccinated at the earliest.India has reported over 11 million cases of COVID-19, making it the second-worst affected country in the world after the United States. The country's daily case count has been declining in recent weeks, but experts have warned that the pandemic is far from over and that precautions need to be maintained.\n",
        "In summary, India's Health Ministry has announced that the country's COVID-19 vaccination drive will be expanded to include people over 60 and those over 45 with co-morbidities, covering an additional 270 million people. The decision was taken after a meeting of the National Expert Group on Vaccine Administration for COVID-19, and is a major step towards achieving herd immunity and controlling the spread of the virus in India.\"\"\"    \n",
        "\n",
        "summary = textrank(text)\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NQkJGoQE2g9",
        "outputId": "a8f757b2-7964-4639-e8ed-3cf06adea02c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"The country's daily case count has been declining in recent weeks, but experts have warned that the pandemic is far from over and that precautions need to be maintained.\", 'The decision was taken after a meeting of the National Expert Group on Vaccine Administration for COVID-19, and is a major step towards achieving herd immunity and controlling the spread of the virus in India.', \"In summary, India's Health Ministry has announced that the country's COVID-19 vaccination drive will be expanded to include people over 60 and those over 45 with co-morbidities, covering an additional 270 million people.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def listToString(s):\n",
        " \n",
        "    # initialize an empty string\n",
        "    str1 = \"\"\n",
        " \n",
        "    # traverse in the string\n",
        "    for ele in s:\n",
        "        str1 += ele\n",
        " \n",
        "    # return string\n",
        "    return str1"
      ],
      "metadata": {
        "id": "VRb4fu2JBg0E"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summ= listToString(summary)"
      ],
      "metadata": {
        "id": "d4wd3J5hBuri"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = Rouge()\n",
        "scores = rouge.get_scores(summ, text)\n",
        "print(\"ROUGE Score:\")\n",
        "print(\"Precision: {:.3f}\".format(scores[0]['rouge-1']['p']))\n",
        "print(\"Recall: {:.3f}\".format(scores[0]['rouge-1']['r']))\n",
        "print(\"F1-Score: {:.3f}\".format(scores[0]['rouge-1']['f']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJoQYwYcJsmd",
        "outputId": "3580f821-0843-4b58-ce9c-f546b706c919"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE Score:\n",
            "Precision: 1.000\n",
            "Recall: 0.503\n",
            "F1-Score: 0.670\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def summary_to_sentences(summ):\n",
        "    # Split the summary into sentences using the '.' character as a separator\n",
        "    sentences = summ.split('.')\n",
        "    \n",
        "    # Convert each sentence into a list of words\n",
        "    sentence_lists = [sentence.split() for sentence in sentences]\n",
        "    \n",
        "    return sentence_lists\n",
        "\n",
        "def paragraph_to_wordlist(paragraph):\n",
        "    # Split the paragraph into words using whitespace as a separator\n",
        "    words = paragraph.split()\n",
        "    return words\n",
        "\n",
        "reference_paragraph = text\n",
        "reference_summary = summary_to_sentences(reference_paragraph)\n",
        "predicted_paragraph = summ\n",
        "predicted_summary = paragraph_to_wordlist(predicted_paragraph)\n",
        "\n",
        "score = sentence_bleu(reference_summary, predicted_summary)\n",
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLNp6NT8K43A",
        "outputId": "12bf11f3-35d6-42c5-e493-689d8c7590e9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9088741852620328\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"BLEU Score: {:.3f}\".format(score))"
      ],
      "metadata": {
        "id": "lo9aK4jLLDIm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0a0e12c-315d-4172-9901-d87224d448e8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score: 0.909\n"
          ]
        }
      ]
    }
  ]
}