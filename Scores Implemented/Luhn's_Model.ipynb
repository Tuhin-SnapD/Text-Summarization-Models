{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Luhn's Model\n",
        "\n",
        "**The Luhn Model** is a statistical-based text summarization technique that selects the most relevant sentences based on the frequency of important words in the text. Here are some advantages and disadvantages of using the Luhn Model for text summarization:\n",
        "\n",
        "### Pros:\n",
        "\n",
        "* Easy to implement: The Luhn Model is a simple algorithm that is easy to implement and requires minimal computational resources.\n",
        "\n",
        "* No training data needed: The Luhn Model does not require any training data, as it is based on a statistical analysis of the text.\n",
        "\n",
        "* Good for extractive summarization: The Luhn Model is well-suited for extractive summarization, where the summary is generated by selecting the most relevant sentences from the original text.\n",
        "\n",
        "* Language-independent: The Luhn Model is language-independent, which means it can be applied to any language.\n",
        "\n",
        "### Cons:\n",
        "\n",
        "* Limited to statistical analysis: The Luhn Model relies solely on a statistical analysis of the text and may not be able to capture the semantic meaning of the text.\n",
        "\n",
        "* Limited context awareness: The Luhn Model does not consider the context in which the sentences are used, which can lead to the selection of irrelevant sentences.\n",
        "\n",
        "* Over-reliance on word frequency: The Luhn Model relies heavily on word frequency, which may not always be an accurate indicator of the importance of a sentence.\n",
        "\n",
        "* Limited to single document summarization: The Luhn Model is designed for single document summarization and may not work well for summarizing multiple documents or large sets of data.\n",
        "\n",
        "These are the scores we achieved:\n",
        "\n",
        "    ROUGE Score:\n",
        "    Precision: 0.991\n",
        "    Recall: 0.742\n",
        "    F1-Score: 0.848\n",
        "\n",
        "    BLEU Score: 0.700\n",
        "\n",
        "## References\n",
        "\n",
        "Here are some research papers related to Luhn's algorithm for text summarization:\n",
        "\n",
        "1. \"The automatic creation of literature abstracts\" by H. P. Luhn, in IBM Journal of Research and Development (1958)\n",
        "\n",
        "2. \"Text summarization using Luhn's algorithm\" by H. P. Luhn, in Information Retrieval Techniques for Speech Applications (1996)\n",
        "\n",
        "3. \"Experiments with Luhn's automatic summarizer\" by T. F. Sumner, in Journal of the Association for Computing Machinery (1959)\n",
        "\n",
        "4. \"Combining Luhn's algorithm with latent semantic analysis for text summarization\" by R. S. Kesavan and S. S. Iyengar, in Proceedings of the 2009 International Conference on Advances in Recent Technologies in Communication and Computing\n",
        "\n",
        "These papers describe the original Luhn's algorithm for text summarization, its limitations, and its extensions. The algorithm is based on identifying the most frequent words in a document and selecting the sentences that contain them. This approach is simple and can produce reasonable results, but it has some limitations, such as the lack of understanding of the semantic relationships between words.\n",
        "\n",
        "The later papers explore extensions to the Luhn's algorithm, such as combining it with other techniques, like latent semantic analysis, to improve its performance. These extensions aim to address some of the limitations of the original algorithm and improve its effectiveness in generating high-quality summaries.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "u2FbDXwOKJgP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lTTxV7GxGdUg",
        "outputId": "457f7141-1708-447f-8f1d-673ab872c7d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.22.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.10.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (3.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk) (2022.6.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "!pip install scikit-learn\n",
        "!pip install rouge\n",
        "!pip install nltk\n",
        "from rouge import Rouge \n",
        "import nltk\n",
        "import nltk.translate.bleu_score as bleu\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize, sent_tokenize "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_keywords(text, n_keywords=10):\n",
        "    # Tokenize the text\n",
        "    tokens = text.lower().split()\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Calculate the frequency of each word\n",
        "    freq = Counter(tokens)\n",
        "\n",
        "    # Assign scores to each word based on frequency and position\n",
        "    scores = {word: freq[word] * (i+1) for i, word in enumerate(tokens)}\n",
        "\n",
        "    # Sort the words by score and select the top n_keywords\n",
        "    keywords = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:n_keywords]\n",
        "\n",
        "    # Return the top keywords\n",
        "    return [keyword[0] for keyword in keywords]"
      ],
      "metadata": {
        "id": "ywbshXsGGyON"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMwcWQYTHXBN",
        "outputId": "18f5f4d0-8be1-4fb4-bd38-26f908a64ecd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        " India's Health Ministry has announced that the country's COVID-19 vaccination drive will now be expanded to include people over the age of 60 and those over 45 with co-morbidities. The move is expected to cover an additional 270 million people, making it one of the largest vaccination drives in the world.The decision was taken after a meeting of the National Expert Group on Vaccine Administration for COVID-19 (NEGVAC), which recommended the expansion of the vaccination program. The NEGVAC also suggested that private hospitals may be allowed to administer the vaccine, although the details of this are yet to be finalized.India began its vaccination drive in mid-January, starting with healthcare and frontline workers. Since then, over 13 million doses have been administered across the country. However, the pace of the vaccination drive has been slower than expected, with concerns raised over vaccine hesitancy and logistical challenges.The expansion of the vaccination drive to include the elderly and those with co-morbidities is a major step towards achieving herd immunity and controlling the spread of the virus in India. The Health Ministry has also urged eligible individuals to come forward and get vaccinated at the earliest.India has reported over 11 million cases of COVID-19, making it the second-worst affected country in the world after the United States. The country's daily case count has been declining in recent weeks, but experts have warned that the pandemic is far from over and that precautions need to be maintained.\n",
        "In summary, India's Health Ministry has announced that the country's COVID-19 vaccination drive will be expanded to include people over 60 and those over 45 with co-morbidities, covering an additional 270 million people. The decision was taken after a meeting of the National Expert Group on Vaccine Administration for COVID-19, and is a major step towards achieving herd immunity and controlling the spread of the virus in India.\"\"\"\n",
        "\n",
        "# Extract the top 3 keywords\n",
        "keywords = extract_keywords(text, n_keywords=3)\n",
        "\n",
        "# Print the keywords\n",
        "print('Top keywords:', keywords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAjkoCdpG2YU",
        "outputId": "94af4f4a-c638-4d8c-d178-a610567524e3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top keywords: ['vaccination', 'drive', 'million']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize the text using the top keywords\n",
        "sentences = text.split('.')\n",
        "summary = ''\n",
        "for sentence in sentences:\n",
        "    for keyword in keywords:\n",
        "        if keyword in sentence.lower():\n",
        "            summary += sentence.strip() + '. '\n",
        "            break\n",
        "\n",
        "# Print the summary\n",
        "print('Summary:', summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_Yfoy2mHUKr",
        "outputId": "1f238863-a3f8-4e73-bf30-0ad021aec397"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary: India's Health Ministry has announced that the country's COVID-19 vaccination drive will now be expanded to include people over the age of 60 and those over 45 with co-morbidities. The move is expected to cover an additional 270 million people, making it one of the largest vaccination drives in the world. The decision was taken after a meeting of the National Expert Group on Vaccine Administration for COVID-19 (NEGVAC), which recommended the expansion of the vaccination program. India began its vaccination drive in mid-January, starting with healthcare and frontline workers. Since then, over 13 million doses have been administered across the country. However, the pace of the vaccination drive has been slower than expected, with concerns raised over vaccine hesitancy and logistical challenges. The expansion of the vaccination drive to include the elderly and those with co-morbidities is a major step towards achieving herd immunity and controlling the spread of the virus in India. India has reported over 11 million cases of COVID-19, making it the second-worst affected country in the world after the United States. In summary, India's Health Ministry has announced that the country's COVID-19 vaccination drive will be expanded to include people over 60 and those over 45 with co-morbidities, covering an additional 270 million people. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = Rouge()\n",
        "scores = rouge.get_scores(summary, text)\n",
        "print(\"ROUGE Score:\")\n",
        "print(\"Precision: {:.3f}\".format(scores[0]['rouge-1']['p']))\n",
        "print(\"Recall: {:.3f}\".format(scores[0]['rouge-1']['r']))\n",
        "print(\"F1-Score: {:.3f}\".format(scores[0]['rouge-1']['f']))"
      ],
      "metadata": {
        "id": "mR7GDwtjHzfB",
        "outputId": "2bc41f67-b736-4be3-ac56-ea38845db6e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE Score:\n",
            "Precision: 0.991\n",
            "Recall: 0.742\n",
            "F1-Score: 0.848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def summary_to_sentences(summary):\n",
        "    # Split the summary into sentences using the '.' character as a separator\n",
        "    sentences = summary.split('.')\n",
        "    \n",
        "    # Convert each sentence into a list of words\n",
        "    sentence_lists = [sentence.split() for sentence in sentences]\n",
        "    \n",
        "    return sentence_lists\n",
        "\n",
        "def paragraph_to_wordlist(paragraph):\n",
        "    # Split the paragraph into words using whitespace as a separator\n",
        "    words = paragraph.split()\n",
        "    return words\n",
        "\n",
        "reference_paragraph = text\n",
        "reference_summary = summary_to_sentences(reference_paragraph)\n",
        "predicted_paragraph = summary\n",
        "predicted_summary = paragraph_to_wordlist(predicted_paragraph)\n",
        "\n",
        "score = sentence_bleu(reference_summary, predicted_summary)\n",
        "print(score)"
      ],
      "metadata": {
        "id": "1SOUmCEXKnZ3",
        "outputId": "12ef2d22-f95a-45bb-90a6-edb2f67f89a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7003175301310649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"BLEU Score: {:.3f}\".format(score))"
      ],
      "metadata": {
        "id": "MyoGCN5KK61U",
        "outputId": "671d94b2-fd8f-462f-eecd-bf2d45aad7cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score: 0.700\n"
          ]
        }
      ]
    }
  ]
}