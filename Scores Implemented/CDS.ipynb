{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTG7KWVOP_aZ",
        "outputId": "e4baf06f-4acb-4be2-c329-1a6a6d0011a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge\n",
        "!pip install nltk\n",
        "import networkx as nx\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from rouge import Rouge \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "import nltk.translate.bleu_score as bleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSTScWFlRbK4",
        "outputId": "4a873fef-8dc1-4280-f269-821cf693edac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_FrNEG0fQ_fR"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocess a given text by tokenizing, removing stop words, and lemmatizing the words.\n",
        "    \"\"\"\n",
        "    # tokenize the text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # remove stop words and lemmatize the words in each sentence\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    preprocessed_sentences = []\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence.lower())\n",
        "        filtered_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "        preprocessed_sentence = \" \".join(filtered_words)\n",
        "        preprocessed_sentences.append(preprocessed_sentence)\n",
        "\n",
        "    return preprocessed_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QOq-xHNrQ_3-"
      },
      "outputs": [],
      "source": [
        "def compute_similarity(sentence1, sentence2):\n",
        "    \"\"\"\n",
        "    Compute the similarity score between two sentences using TF-IDF.\n",
        "    \"\"\"\n",
        "    tfidf = TfidfVectorizer().fit_transform([sentence1, sentence2])\n",
        "    similarity_score = (tfidf * tfidf.T).A[0, 1]\n",
        "    return similarity_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "85qyfQEvRCIE"
      },
      "outputs": [],
      "source": [
        "def find_minimum_cds(graph):\n",
        "    \"\"\"\n",
        "    Find the minimum Connected Dominating Set (CDS) of a graph using a greedy algorithm.\n",
        "    \"\"\"\n",
        "    cds = set() # initialize CDS to empty set\n",
        "    nodes = set(graph.nodes()) # get all nodes in the graph\n",
        "\n",
        "    while nodes:\n",
        "        max_degree_node = max(nodes, key=lambda n: graph.degree(n)) # find node with highest degree\n",
        "        cds.add(max_degree_node) # add node to CDS\n",
        "        nodes.discard(max_degree_node) # remove node from remaining nodes\n",
        "        neighbors = set(graph.neighbors(max_degree_node)) # get all neighbors of the node\n",
        "        nodes.difference_update(neighbors) # remove neighbors from remaining nodes\n",
        "\n",
        "    return cds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jA-5S0DxREBP"
      },
      "outputs": [],
      "source": [
        "def summarize_text(text, summary_size, threshold=0.1):\n",
        "    \"\"\"\n",
        "    Summarize a given text using minimum Connected Dominating Set (CDS).\n",
        "    \"\"\"\n",
        "    # preprocess the text\n",
        "    preprocessed_sentences = preprocess_text(text)\n",
        "\n",
        "    # create graph from preprocessed sentences\n",
        "    graph = nx.Graph()\n",
        "    for i, sentence in enumerate(preprocessed_sentences):\n",
        "        for j in range(i+1, len(preprocessed_sentences)):\n",
        "            similarity_score = compute_similarity(sentence, preprocessed_sentences[j]) # compute similarity score between two sentences\n",
        "            if similarity_score > threshold:\n",
        "                graph.add_edge(i, j, weight=similarity_score)\n",
        "\n",
        "    # find minimum CDS of the graph\n",
        "    cds = find_minimum_cds(graph)\n",
        "\n",
        "    # sort the CDS nodes based on their occurrence order in the original text\n",
        "    summary_nodes = sorted(list(cds))\n",
        "\n",
        "    # create summary by concatenating the selected sentences\n",
        "    summary = \". \".join([sent_tokenize(text)[i] for i in summary_nodes][:summary_size])\n",
        "\n",
        "    return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTepjNBWRGUf",
        "outputId": "15ab2aeb-4eb2-4e23-eaf0-fbb0489aa68b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The move is expected to cover an additional 270 million people, making it one of the largest vaccination drives in the world.The decision was taken after a meeting of the National Expert Group on Vaccine Administration for COVID-19 (NEGVAC), which recommended the expansion of the vaccination program.. The Health Ministry has also urged eligible individuals to come forward and get vaccinated at the earliest.India has reported over 11 million cases of COVID-19, making it the second-worst affected country in the world after the United States.\n"
          ]
        }
      ],
      "source": [
        "text =\"\"\"\n",
        " India's Health Ministry has announced that the country's COVID-19 vaccination drive will now be expanded to include people over the age of 60 and those over 45 with co-morbidities. The move is expected to cover an additional 270 million people, making it one of the largest vaccination drives in the world.The decision was taken after a meeting of the National Expert Group on Vaccine Administration for COVID-19 (NEGVAC), which recommended the expansion of the vaccination program. The NEGVAC also suggested that private hospitals may be allowed to administer the vaccine, although the details of this are yet to be finalized.India began its vaccination drive in mid-January, starting with healthcare and frontline workers. Since then, over 13 million doses have been administered across the country. However, the pace of the vaccination drive has been slower than expected, with concerns raised over vaccine hesitancy and logistical challenges.The expansion of the vaccination drive to include the elderly and those with co-morbidities is a major step towards achieving herd immunity and controlling the spread of the virus in India. The Health Ministry has also urged eligible individuals to come forward and get vaccinated at the earliest.India has reported over 11 million cases of COVID-19, making it the second-worst affected country in the world after the United States. The country's daily case count has been declining in recent weeks, but experts have warned that the pandemic is far from over and that precautions need to be maintained.\n",
        "In summary, India's Health Ministry has announced that the country's COVID-19 vaccination drive will be expanded to include people over 60 and those over 45 with co-morbidities, covering an additional 270 million people. The decision was taken after a meeting of the National Expert Group on Vaccine Administration for COVID-19, and is a major step towards achieving herd immunity and controlling the spread of the virus in India.\"\"\"\n",
        "\n",
        "summary_size = 3 # number of sentences in the summary\n",
        "summary = summarize_text(text, summary_size)\n",
        "\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcRB4Cn0R_UA",
        "outputId": "2b9d3019-c6f8-4bb0-afde-d3e89e99a82b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROUGE Score:\n",
            "Precision: 1.000\n",
            "Recall: 0.430\n",
            "F1-Score: 0.602\n"
          ]
        }
      ],
      "source": [
        "rouge = Rouge()\n",
        "scores = rouge.get_scores(summary, text)\n",
        "print(\"ROUGE Score:\")\n",
        "print(\"Precision: {:.3f}\".format(scores[0]['rouge-1']['p']))\n",
        "print(\"Recall: {:.3f}\".format(scores[0]['rouge-1']['r']))\n",
        "print(\"F1-Score: {:.3f}\".format(scores[0]['rouge-1']['f']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2n8nozo0XmKJ",
        "outputId": "3fc8f3fb-4169-4458-d567-68842b70146b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8435083039960267\n"
          ]
        }
      ],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def summary_to_sentences(summary):\n",
        "    # Split the summary into sentences using the '.' character as a separator\n",
        "    sentences = summary.split('.')\n",
        "    \n",
        "    # Convert each sentence into a list of words\n",
        "    sentence_lists = [sentence.split() for sentence in sentences]\n",
        "    \n",
        "    return sentence_lists\n",
        "\n",
        "def paragraph_to_wordlist(paragraph):\n",
        "    # Split the paragraph into words using whitespace as a separator\n",
        "    words = paragraph.split()\n",
        "    return words\n",
        "\n",
        "reference_paragraph = text\n",
        "reference_summary = summary_to_sentences(reference_paragraph)\n",
        "predicted_paragraph = summary\n",
        "predicted_summary = paragraph_to_wordlist(predicted_paragraph)\n",
        "\n",
        "score = sentence_bleu(reference_summary, predicted_summary)\n",
        "print(score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lHe3peYXoRL",
        "outputId": "1b99d191-a4bd-4880-e4fd-6be470c3a746"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU Score: 0.844\n"
          ]
        }
      ],
      "source": [
        "print(\"BLEU Score: {:.3f}\".format(score))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
