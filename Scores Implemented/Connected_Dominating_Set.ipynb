{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CDS\n",
        "Text summarization using **Connected Dominating Set (CDS)** is a technique that involves selecting the most important sentences from a text document to create a shorter version of the original. Here are some advantages and disadvantages of using CDS for text summarization:\n",
        "\n",
        "### Pros:\n",
        "\n",
        "* Good Coverage: CDS-based summarization techniques can cover most of the important topics in a document, as they aim to select the most representative sentences that cover the main themes and ideas.\n",
        "\n",
        "* Improved Coherence: CDS-based techniques tend to produce summaries that are more coherent than other methods, as they select sentences that are more connected to each other in terms of content.\n",
        "\n",
        "* Speed: CDS-based techniques are relatively fast and can generate summaries quickly, making them suitable for summarizing large volumes of text.\n",
        "\n",
        "* Flexibility: CDS-based techniques can be adapted to different types of text documents, including news articles, research papers, and other types of documents.\n",
        "\n",
        "### Cons:\n",
        "\n",
        "* Limited Precision: CDS-based summarization techniques may not always select the most important sentences from a document, as they focus more on coverage and coherence rather than precision.\n",
        "\n",
        "* Subjectivity: CDS-based techniques can be subjective, as the selection of the most important sentences can vary depending on the criteria used to define importance.\n",
        "\n",
        "* Lack of Context: CDS-based techniques may not take into account the context of a sentence, which can lead to the selection of sentences that are not relevant to the main theme or idea.\n",
        "\n",
        "* Over-simplification: CDS-based techniques can oversimplify complex documents, as they tend to focus on the most important sentences and may leave out important details or nuances.\n",
        "\n",
        "These are the scores we achieved:\n",
        "\n",
        "    ROUGE Score:\n",
        "    Precision: 1.000\n",
        "    Recall: 0.430\n",
        "    F1-Score: 0.602\n",
        "\n",
        "    BLEU Score: 0.844\n",
        "\n",
        "## References \n",
        "\n",
        "1. \"A new approach for text summarization using connected dominating set in graphs\" by M. Sadeghi and M. M. Farsangi, in Proceedings of the 2010 International Conference on Computer, Mechatronics, Control and Electronic Engineering (CMCE)\n",
        "\n",
        "2. \"Text summarization using a graph-based method with connected dominating set\" by A. E. Bayraktar and F. Can, in Proceedings of the 2012 International Conference on Computer Science and Engineering (UBMK)\n",
        "\n",
        "3. \"Extractive text summarization based on the connected dominating set in a graph representation\" by A. E. Bayraktar and F. Can, in Turkish Journal of Electrical Engineering & Computer Sciences\n",
        "\n",
        "4. \"A novel text summarization technique based on connected dominating set in graph\" by M. Sadeghi and M. M. Farsangi, in the Journal of Information Science and Engineering\n",
        "\n",
        "These papers propose using the CDS algorithm to build a graph-based representation of the document and then extracting the summary by selecting the most important sentences or nodes in the CDS. The CDS approach has been shown to be effective in identifying the most important nodes in the graph, and can lead to high-quality summaries."
      ],
      "metadata": {
        "id": "4Y_HUXHGG9Gs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTG7KWVOP_aZ",
        "outputId": "e4baf06f-4acb-4be2-c329-1a6a6d0011a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge\n",
        "!pip install nltk\n",
        "import networkx as nx\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from rouge import Rouge \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "import nltk.translate.bleu_score as bleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSTScWFlRbK4",
        "outputId": "4a873fef-8dc1-4280-f269-821cf693edac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FrNEG0fQ_fR"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocess a given text by tokenizing, removing stop words, and lemmatizing the words.\n",
        "    \"\"\"\n",
        "    # tokenize the text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # remove stop words and lemmatize the words in each sentence\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    preprocessed_sentences = []\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence.lower())\n",
        "        filtered_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "        preprocessed_sentence = \" \".join(filtered_words)\n",
        "        preprocessed_sentences.append(preprocessed_sentence)\n",
        "\n",
        "    return preprocessed_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOq-xHNrQ_3-"
      },
      "outputs": [],
      "source": [
        "def compute_similarity(sentence1, sentence2):\n",
        "    \"\"\"\n",
        "    Compute the similarity score between two sentences using TF-IDF.\n",
        "    \"\"\"\n",
        "    tfidf = TfidfVectorizer().fit_transform([sentence1, sentence2])\n",
        "    similarity_score = (tfidf * tfidf.T).A[0, 1]\n",
        "    return similarity_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85qyfQEvRCIE"
      },
      "outputs": [],
      "source": [
        "def find_minimum_cds(graph):\n",
        "    \"\"\"\n",
        "    Find the minimum Connected Dominating Set (CDS) of a graph using a greedy algorithm.\n",
        "    \"\"\"\n",
        "    cds = set() # initialize CDS to empty set\n",
        "    nodes = set(graph.nodes()) # get all nodes in the graph\n",
        "\n",
        "    while nodes:\n",
        "        max_degree_node = max(nodes, key=lambda n: graph.degree(n)) # find node with highest degree\n",
        "        cds.add(max_degree_node) # add node to CDS\n",
        "        nodes.discard(max_degree_node) # remove node from remaining nodes\n",
        "        neighbors = set(graph.neighbors(max_degree_node)) # get all neighbors of the node\n",
        "        nodes.difference_update(neighbors) # remove neighbors from remaining nodes\n",
        "\n",
        "    return cds"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BR_cl2XyEpy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jA-5S0DxREBP"
      },
      "outputs": [],
      "source": [
        "def summarize_text(text, summary_size, threshold=0.1):\n",
        "    \"\"\"\n",
        "    Summarize a given text using minimum Connected Dominating Set (CDS).\n",
        "    \"\"\"\n",
        "    # preprocess the text\n",
        "    preprocessed_sentences = preprocess_text(text)\n",
        "\n",
        "    # create graph from preprocessed sentences\n",
        "    graph = nx.Graph()\n",
        "    for i, sentence in enumerate(preprocessed_sentences):\n",
        "        for j in range(i+1, len(preprocessed_sentences)):\n",
        "            similarity_score = compute_similarity(sentence, preprocessed_sentences[j]) # compute similarity score between two sentences\n",
        "            if similarity_score > threshold:\n",
        "                graph.add_edge(i, j, weight=similarity_score)\n",
        "\n",
        "    # find minimum CDS of the graph\n",
        "    cds = find_minimum_cds(graph)\n",
        "\n",
        "    # sort the CDS nodes based on their occurrence order in the original text\n",
        "    summary_nodes = sorted(list(cds))\n",
        "\n",
        "    # create summary by concatenating the selected sentences\n",
        "    summary = \". \".join([sent_tokenize(text)[i] for i in summary_nodes][:summary_size])\n",
        "\n",
        "    return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTepjNBWRGUf",
        "outputId": "15ab2aeb-4eb2-4e23-eaf0-fbb0489aa68b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The move is expected to cover an additional 270 million people, making it one of the largest vaccination drives in the world.The decision was taken after a meeting of the National Expert Group on Vaccine Administration for COVID-19 (NEGVAC), which recommended the expansion of the vaccination program.. The Health Ministry has also urged eligible individuals to come forward and get vaccinated at the earliest.India has reported over 11 million cases of COVID-19, making it the second-worst affected country in the world after the United States.\n"
          ]
        }
      ],
      "source": [
        "text =\"\"\"\n",
        " India's Health Ministry has announced that the country's COVID-19 vaccination drive will now be expanded to include people over the age of 60 and those over 45 with co-morbidities. The move is expected to cover an additional 270 million people, making it one of the largest vaccination drives in the world.The decision was taken after a meeting of the National Expert Group on Vaccine Administration for COVID-19 (NEGVAC), which recommended the expansion of the vaccination program. The NEGVAC also suggested that private hospitals may be allowed to administer the vaccine, although the details of this are yet to be finalized.India began its vaccination drive in mid-January, starting with healthcare and frontline workers. Since then, over 13 million doses have been administered across the country. However, the pace of the vaccination drive has been slower than expected, with concerns raised over vaccine hesitancy and logistical challenges.The expansion of the vaccination drive to include the elderly and those with co-morbidities is a major step towards achieving herd immunity and controlling the spread of the virus in India. The Health Ministry has also urged eligible individuals to come forward and get vaccinated at the earliest.India has reported over 11 million cases of COVID-19, making it the second-worst affected country in the world after the United States. The country's daily case count has been declining in recent weeks, but experts have warned that the pandemic is far from over and that precautions need to be maintained.\n",
        "In summary, India's Health Ministry has announced that the country's COVID-19 vaccination drive will be expanded to include people over 60 and those over 45 with co-morbidities, covering an additional 270 million people. The decision was taken after a meeting of the National Expert Group on Vaccine Administration for COVID-19, and is a major step towards achieving herd immunity and controlling the spread of the virus in India.\"\"\"\n",
        "\n",
        "summary_size = 3 # number of sentences in the summary\n",
        "summary = summarize_text(text, summary_size)\n",
        "\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcRB4Cn0R_UA",
        "outputId": "2b9d3019-c6f8-4bb0-afde-d3e89e99a82b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROUGE Score:\n",
            "Precision: 1.000\n",
            "Recall: 0.430\n",
            "F1-Score: 0.602\n"
          ]
        }
      ],
      "source": [
        "rouge = Rouge()\n",
        "scores = rouge.get_scores(summary, text)\n",
        "print(\"ROUGE Score:\")\n",
        "print(\"Precision: {:.3f}\".format(scores[0]['rouge-1']['p']))\n",
        "print(\"Recall: {:.3f}\".format(scores[0]['rouge-1']['r']))\n",
        "print(\"F1-Score: {:.3f}\".format(scores[0]['rouge-1']['f']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2n8nozo0XmKJ",
        "outputId": "3fc8f3fb-4169-4458-d567-68842b70146b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8435083039960267\n"
          ]
        }
      ],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def summary_to_sentences(summary):\n",
        "    # Split the summary into sentences using the '.' character as a separator\n",
        "    sentences = summary.split('.')\n",
        "    \n",
        "    # Convert each sentence into a list of words\n",
        "    sentence_lists = [sentence.split() for sentence in sentences]\n",
        "    \n",
        "    return sentence_lists\n",
        "\n",
        "def paragraph_to_wordlist(paragraph):\n",
        "    # Split the paragraph into words using whitespace as a separator\n",
        "    words = paragraph.split()\n",
        "    return words\n",
        "\n",
        "reference_paragraph = text\n",
        "reference_summary = summary_to_sentences(reference_paragraph)\n",
        "predicted_paragraph = summary\n",
        "predicted_summary = paragraph_to_wordlist(predicted_paragraph)\n",
        "\n",
        "score = sentence_bleu(reference_summary, predicted_summary)\n",
        "print(score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lHe3peYXoRL",
        "outputId": "1b99d191-a4bd-4880-e4fd-6be470c3a746"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU Score: 0.844\n"
          ]
        }
      ],
      "source": [
        "print(\"BLEU Score: {:.3f}\".format(score))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}